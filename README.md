# ğŸ¤– NLP Lab 2 â€” Fine-Tuning Transformers for Text Classification

This notebook is part of a university course on Natural Language Processing and demonstrates how to fine-tune a pretrained transformer-based language model for text classification using the Hugging Face ecosystem.

---

## ğŸš€ Whatâ€™s Inside

- âœï¸ Introduction to Pretraining vs Fine-tuning
- ğŸ“Š Task: Text Classification (e.g. NLI or sentiment analysis)
- ğŸ§  Using Hugging Face `transformers`, `datasets`, `evaluate`
- ğŸ› ï¸ Custom training loop or use of `Trainer`
- ğŸ“ˆ Evaluation with accuracy, precision, recall, F1

---

## ğŸ§ª Technologies Used

- Python
- Hugging Face Transformers
- Datasets (Hugging Face)
- Evaluate
- Accelerate
- Sentence-Transformers
- Matplotlib

---

## ğŸ“ Requirements

Make sure to install the necessary libraries before running the notebook:

```bash
pip install transformers datasets evaluate accelerate sentence-transformers matplotlib
