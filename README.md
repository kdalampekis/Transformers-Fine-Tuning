# 🤖 NLP Lab 2 — Fine-Tuning Transformers for Text Classification

This notebook is part of a university course on Natural Language Processing and demonstrates how to fine-tune a pretrained transformer-based language model for text classification using the Hugging Face ecosystem.

---

## 🚀 What’s Inside

- ✍️ Introduction to Pretraining vs Fine-tuning
- 📊 Task: Text Classification (e.g. NLI or sentiment analysis)
- 🧠 Using Hugging Face `transformers`, `datasets`, `evaluate`
- 🛠️ Custom training loop or use of `Trainer`
- 📈 Evaluation with accuracy, precision, recall, F1

---

## 🧪 Technologies Used

- Python
- Hugging Face Transformers
- Datasets (Hugging Face)
- Evaluate
- Accelerate
- Sentence-Transformers
- Matplotlib

---

## 📎 Requirements

Make sure to install the necessary libraries before running the notebook:

```bash
pip install transformers datasets evaluate accelerate sentence-transformers matplotlib
